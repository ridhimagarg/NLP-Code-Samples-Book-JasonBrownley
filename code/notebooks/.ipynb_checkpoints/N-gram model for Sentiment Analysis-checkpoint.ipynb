{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "from pickle import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "process_docs() missing 1 required positional argument: 'is_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-3af1c02740cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m \u001b[0mtrain_docs\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_clean_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[0mtest_docs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_clean_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-3af1c02740cb>\u001b[0m in \u001b[0;36mload_clean_dataset\u001b[1;34m(is_train)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_clean_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_docs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../../data/input/txt_sentoken/pos'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[0mneg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_docs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../../data/input/txt_sentoken/neg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: process_docs() missing 1 required positional argument: 'is_train'"
     ]
    }
   ],
   "source": [
    "def load_doc(filename):\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        \n",
    "        text = f.read()\n",
    "        \n",
    "    f.close()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_doc(doc):\n",
    "    \n",
    "    tokens = doc.split()\n",
    "    \n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    \n",
    "    tokens = [re_punc.sub('', w) for word in tokens]\n",
    "    \n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    tokens = [word for word in tokens if len(word)> 1]\n",
    "    \n",
    "    tokens = ' '.join(tokens)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def process_docs(directory, is_train):\n",
    "    \n",
    "    documents = list()\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        \n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "            \n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "            \n",
    "        path = directory + '/' + filename\n",
    "        \n",
    "        doc = load_doc(path)\n",
    "        \n",
    "        tokens = clean_doc(doc)\n",
    "        \n",
    "        documents.append(tokens)\n",
    "\n",
    "def load_clean_dataset(is_train):\n",
    "    \n",
    "    pos = process_docs('../../data/input/txt_sentoken/pos', is_train)\n",
    "    neg = process_docs('../../data/input/txt_sentoken/neg', is_train)\n",
    "    \n",
    "    docs = pos + neg\n",
    "    \n",
    "    labels = [1 for _ in range(len(psos))] + [0 for _ in range(len(neg))]\n",
    "    \n",
    "    return docs, labels\n",
    "\n",
    "def save_dataset(dataset, filename):\n",
    "    dump(dataset, open(filename, 'wb'))\n",
    "    \n",
    "    print('Saved %s' % filename)\n",
    "    \n",
    "\n",
    "train_docs , ytrain = load_clean_dataset(True)\n",
    "test_docs, ytest = load_clean_dataset(False)\n",
    "\n",
    "save_dataset([train_docs, ytrain], 'train.pkl')\n",
    "save_dataset([test_docs, ytest], 'test.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
